import ssl
import asyncio
import threading
import queue
import time
import os
import re
import numpy as np
import sounddevice as sd
import whisper
from dotenv import load_dotenv
from openai import AsyncOpenAI

# --- SSL fix –¥–ª—è macOS ---
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# --- –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è ---
SAMPLE_RATE = 16000
BLOCK_SECONDS = 5
DEVICE_NAME = "BlackHole 2ch"
CONTEXT_SIZE = 3  # –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö —Ä–µ—á–µ–Ω—å –¥–ª—è GPT

# --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è .env ---
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = AsyncOpenAI(api_key=OPENAI_API_KEY)

# --- –ß–µ—Ä–≥–∏ ---
audio_queue = queue.Queue()
transcribe_queue = queue.Queue()
text_queue = asyncio.Queue()

# --- –ë—É—Ñ–µ—Ä –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Ä–µ—á–µ–Ω—å ---
sentence_buffer = []

# --- –õ–æ–≥ –∑ –º—ñ—Ç–∫–æ—é —á–∞—Å—É ---
def log(msg):
    print(f"[LOG {time.strftime('%H:%M:%S')}]: {msg}")

# --- –ü—Ä–æ—Å—Ç–∞ —Ñ—É–Ω–∫—Ü—ñ—è —Ä–æ–∑–±–∏—Ç—Ç—è –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è ---
def split_into_sentences(text):
    sentence_endings = re.compile(r'(?<=[.!?])\s+|\n+|‚Äî ')
    return sentence_endings.split(text.strip())

# --- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ Whisper ---
log("üîÅ Loading Whisper model...")
model = whisper.load_model("small")
log("‚úÖ Whisper model loaded.")

# --- Callback –∞—É–¥—ñ–æ –ø–æ—Ç–æ–∫—É ---
def audio_callback(indata, frames, time_info, status):
    if status:
        log(f"‚ö†Ô∏è Audio status: {status}")
    audio_queue.put(indata.copy())

# --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞: —á–∏ —î –ø–∏—Ç–∞–Ω–Ω—è–º? ---
async def is_question_openai_async(text: str) -> bool:
    prompt = f"Decide if the following text is a question. Answer only 'yes' or 'no'.\n\nText: \"{text}\""
    try:
        log(f"‚Üí GPT prompt: {text}")
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "You classify if the input text is a question."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=3,
            temperature=0,
        )
        answer = response.choices[0].message.content.strip().lower()
        log(f"‚Üê GPT response: {answer}")
        return answer.startswith("yes")
    except Exception as e:
        log(f"OpenAI API error: {e}")
        return False

# --- –û–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—ñ–≤ ---
async def process_texts():
    while True:
        text = await text_queue.get()
        if text is None:
            break
        log(f"üìÑ Received transcription: {text}")
        sentences = split_into_sentences(text)
        log(f"‚úÇÔ∏è Split into sentences: {sentences}")

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            # –î–æ–¥–∞—î–º–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏–π –±—É—Ñ–µ—Ä
            sentence_buffer.append(sentence)
            if len(sentence_buffer) > CONTEXT_SIZE:
                sentence_buffer.pop(0)

            context_text = " ".join(sentence_buffer)
            log(f"ü§î Checking if question (context): {context_text}")
            is_question = await is_question_openai_async(context_text)

            if is_question:
                print(f"\n‚ùì Question detected: {sentence}\n")

# --- –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –∞—É–¥—ñ–æ (–≤ –æ–∫—Ä–µ–º–æ–º—É –ø–æ—Ç–æ—Ü—ñ) ---
def transcribe_worker():
    while True:
        audio_chunk = transcribe_queue.get()
        if audio_chunk is None:
            break
        log("üîä Transcribing audio chunk...")
        result = model.transcribe(audio_chunk, fp16=False, language="uk")
        text = result["text"].strip()
        if text:
            log(f"üìù Transcription result: {text}")
            asyncio.run_coroutine_threadsafe(text_queue.put(text), async_loop)

# --- Async loop —É —Ñ–æ–Ω–æ–≤–æ–º—É –ø–æ—Ç–æ—Ü—ñ ---
def start_async_loop(loop):
    asyncio.set_event_loop(loop)
    loop.run_forever()

async_loop = asyncio.new_event_loop()
threading.Thread(target=start_async_loop, args=(async_loop,), daemon=True).start()
asyncio.run_coroutine_threadsafe(process_texts(), async_loop)

# --- –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –∞—É–¥—ñ–æ-–ø—Ä–∏—Å—Ç—Ä–æ—é ---
device_index = None
for idx, dev in enumerate(sd.query_devices()):
    if DEVICE_NAME in dev['name']:
        device_index = idx
        break
if device_index is None:
    raise RuntimeError(f"Device '{DEVICE_NAME}' not found")
log(f"üéß Using device: {DEVICE_NAME} (index {device_index})")

# --- –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫—É —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó ---
threading.Thread(target=transcribe_worker, daemon=True).start()

# --- –û—Å–Ω–æ–≤–Ω–∏–π —Ü–∏–∫–ª –ø—Ä–æ—Å–ª—É—Ö–æ–≤—É–≤–∞–Ω–Ω—è ---
with sd.InputStream(
    samplerate=SAMPLE_RATE,
    channels=1,
    dtype='float32',
    callback=audio_callback,
    device=device_index
):
    buffer = np.empty((0, 1), dtype='float32')
    last_time = time.time()
    log("‚úÖ Listening... Press Ctrl+C to stop.")

    try:
        while True:
            data = audio_queue.get()
            buffer = np.append(buffer, data, axis=0)
            if time.time() - last_time >= BLOCK_SECONDS:
                last_time = time.time()
                if len(buffer) == 0:
                    continue
                audio_chunk = buffer.flatten()
                buffer = np.empty((0, 1), dtype='float32')
                transcribe_queue.put(audio_chunk)
    except KeyboardInterrupt:
        log("‚èπÔ∏è Stopping...")
        transcribe_queue.put(None)
        asyncio.run_coroutine_threadsafe(text_queue.put(None), async_loop)
        async_loop.call_soon_threadsafe(async_loop.stop)
